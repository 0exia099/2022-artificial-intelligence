{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"colab_09_Wine_Checkpoint.ipynb","provenance":[{"file_id":"1N7b6A2FrMgd84BAKWgoHpbuhkc9ahu4h","timestamp":1585420853543}]}},"cells":[{"cell_type":"code","metadata":{"id":"tDufv2Hbb4xf","outputId":"6046d482-5613-4402-d5ce-a91275c9870a","executionInfo":{"status":"ok","timestamp":1657856757774,"user_tz":-540,"elapsed":86877,"user":{"displayName":"차세대컴퓨터시스템연구실","userId":"00707404389080078471"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":1000}},"source":["# 데이터 입력\n","from google.colab import files\n","uploaded = files.upload()\n","my_data = 'wine.csv'\n","\n","!pip install -q tensorflow-gpu==1.15.0\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import pandas as pd\n","import numpy\n","import os\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint\n","\n","# seed 값 설정\n","numpy.random.seed(3)\n","tf.compat.v1.set_random_seed(3)\n","\n","# 데이터 적용\n","df_pre = pd.read_csv(my_data, header=None)\n","df = df_pre.sample(frac=1)\n","\n","dataset = df.values\n","X = dataset[:,0:12]\n","Y = dataset[:,12]\n","\n","# 모델의 설정\n","model = Sequential()\n","model.add(Dense(30,  input_dim=12, activation='relu'))\n","model.add(Dense(12, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# 모델 컴파일\n","model.compile(loss='binary_crossentropy',\n","          optimizer='adam',\n","          metrics=['accuracy'])\n","\n","# 모델 저장 폴더 설정\n","MODEL_DIR = './model/'\n","if not os.path.exists(MODEL_DIR):\n","   os.mkdir(MODEL_DIR)\n","\n","# 모델 저장 조건 설정\n","modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n","\n","# 모델 실행 및 저장\n","model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])\n"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-503a2a7a-18d8-4e50-8275-d6b52217962e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-503a2a7a-18d8-4e50-8275-d6b52217962e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving wine.csv to wine.csv\n","\u001b[K     |████████████████████████████████| 411.5 MB 7.5 kB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 37.2 MB/s \n","\u001b[K     |████████████████████████████████| 503 kB 55.1 MB/s \n","\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n","\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.2+zzzcolab20220527125636 requires tensorboard<2.9,>=2.8, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow 2.8.2+zzzcolab20220527125636 requires tensorflow-estimator<2.9,>=2.8, but you have tensorflow-estimator 1.15.1 which is incompatible.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","WARNING: Tensorflow 1 is deprecated, and support will be removed on August 1, 2022.\n","After that, `%tensorflow_version 1.x` will throw an error.\n","\n","Your notebook should be updated to use Tensorflow 2.\n","See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.\n","\n","TensorFlow 1.x selected.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"]},{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","\n","Epoch 00001: val_loss improved from inf to 0.35969, saving model to ./model/01-0.3597.hdf5\n","\n","Epoch 00002: val_loss improved from 0.35969 to 0.31442, saving model to ./model/02-0.3144.hdf5\n","\n","Epoch 00003: val_loss improved from 0.31442 to 0.29048, saving model to ./model/03-0.2905.hdf5\n","\n","Epoch 00004: val_loss improved from 0.29048 to 0.27230, saving model to ./model/04-0.2723.hdf5\n","\n","Epoch 00005: val_loss improved from 0.27230 to 0.24911, saving model to ./model/05-0.2491.hdf5\n","\n","Epoch 00006: val_loss improved from 0.24911 to 0.23073, saving model to ./model/06-0.2307.hdf5\n","\n","Epoch 00007: val_loss improved from 0.23073 to 0.21586, saving model to ./model/07-0.2159.hdf5\n","\n","Epoch 00008: val_loss improved from 0.21586 to 0.20313, saving model to ./model/08-0.2031.hdf5\n","\n","Epoch 00009: val_loss improved from 0.20313 to 0.19348, saving model to ./model/09-0.1935.hdf5\n","\n","Epoch 00010: val_loss improved from 0.19348 to 0.18591, saving model to ./model/10-0.1859.hdf5\n","\n","Epoch 00011: val_loss improved from 0.18591 to 0.18198, saving model to ./model/11-0.1820.hdf5\n","\n","Epoch 00012: val_loss did not improve from 0.18198\n","\n","Epoch 00013: val_loss improved from 0.18198 to 0.17508, saving model to ./model/13-0.1751.hdf5\n","\n","Epoch 00014: val_loss improved from 0.17508 to 0.17019, saving model to ./model/14-0.1702.hdf5\n","\n","Epoch 00015: val_loss improved from 0.17019 to 0.16310, saving model to ./model/15-0.1631.hdf5\n","\n","Epoch 00016: val_loss improved from 0.16310 to 0.15970, saving model to ./model/16-0.1597.hdf5\n","\n","Epoch 00017: val_loss improved from 0.15970 to 0.15545, saving model to ./model/17-0.1555.hdf5\n","\n","Epoch 00018: val_loss improved from 0.15545 to 0.15024, saving model to ./model/18-0.1502.hdf5\n","\n","Epoch 00019: val_loss improved from 0.15024 to 0.14620, saving model to ./model/19-0.1462.hdf5\n","\n","Epoch 00020: val_loss improved from 0.14620 to 0.13830, saving model to ./model/20-0.1383.hdf5\n","\n","Epoch 00021: val_loss improved from 0.13830 to 0.13497, saving model to ./model/21-0.1350.hdf5\n","\n","Epoch 00022: val_loss improved from 0.13497 to 0.13100, saving model to ./model/22-0.1310.hdf5\n","\n","Epoch 00023: val_loss improved from 0.13100 to 0.12561, saving model to ./model/23-0.1256.hdf5\n","\n","Epoch 00024: val_loss improved from 0.12561 to 0.12187, saving model to ./model/24-0.1219.hdf5\n","\n","Epoch 00025: val_loss did not improve from 0.12187\n","\n","Epoch 00026: val_loss improved from 0.12187 to 0.12026, saving model to ./model/26-0.1203.hdf5\n","\n","Epoch 00027: val_loss did not improve from 0.12026\n","\n","Epoch 00028: val_loss improved from 0.12026 to 0.11315, saving model to ./model/28-0.1132.hdf5\n","\n","Epoch 00029: val_loss improved from 0.11315 to 0.10934, saving model to ./model/29-0.1093.hdf5\n","\n","Epoch 00030: val_loss improved from 0.10934 to 0.10929, saving model to ./model/30-0.1093.hdf5\n","\n","Epoch 00031: val_loss improved from 0.10929 to 0.10572, saving model to ./model/31-0.1057.hdf5\n","\n","Epoch 00032: val_loss improved from 0.10572 to 0.10203, saving model to ./model/32-0.1020.hdf5\n","\n","Epoch 00033: val_loss improved from 0.10203 to 0.09958, saving model to ./model/33-0.0996.hdf5\n","\n","Epoch 00034: val_loss improved from 0.09958 to 0.09631, saving model to ./model/34-0.0963.hdf5\n","\n","Epoch 00035: val_loss improved from 0.09631 to 0.09464, saving model to ./model/35-0.0946.hdf5\n","\n","Epoch 00036: val_loss improved from 0.09464 to 0.09294, saving model to ./model/36-0.0929.hdf5\n","\n","Epoch 00037: val_loss did not improve from 0.09294\n","\n","Epoch 00038: val_loss improved from 0.09294 to 0.09233, saving model to ./model/38-0.0923.hdf5\n","\n","Epoch 00039: val_loss did not improve from 0.09233\n","\n","Epoch 00040: val_loss improved from 0.09233 to 0.08955, saving model to ./model/40-0.0896.hdf5\n","\n","Epoch 00041: val_loss improved from 0.08955 to 0.08604, saving model to ./model/41-0.0860.hdf5\n","\n","Epoch 00042: val_loss did not improve from 0.08604\n","\n","Epoch 00043: val_loss did not improve from 0.08604\n","\n","Epoch 00044: val_loss did not improve from 0.08604\n","\n","Epoch 00045: val_loss did not improve from 0.08604\n","\n","Epoch 00046: val_loss improved from 0.08604 to 0.08404, saving model to ./model/46-0.0840.hdf5\n","\n","Epoch 00047: val_loss improved from 0.08404 to 0.07999, saving model to ./model/47-0.0800.hdf5\n","\n","Epoch 00048: val_loss improved from 0.07999 to 0.07918, saving model to ./model/48-0.0792.hdf5\n","\n","Epoch 00049: val_loss did not improve from 0.07918\n","\n","Epoch 00050: val_loss did not improve from 0.07918\n","\n","Epoch 00051: val_loss did not improve from 0.07918\n","\n","Epoch 00052: val_loss did not improve from 0.07918\n","\n","Epoch 00053: val_loss did not improve from 0.07918\n","\n","Epoch 00054: val_loss did not improve from 0.07918\n","\n","Epoch 00055: val_loss did not improve from 0.07918\n","\n","Epoch 00056: val_loss improved from 0.07918 to 0.07684, saving model to ./model/56-0.0768.hdf5\n","\n","Epoch 00057: val_loss improved from 0.07684 to 0.07311, saving model to ./model/57-0.0731.hdf5\n","\n","Epoch 00058: val_loss improved from 0.07311 to 0.07223, saving model to ./model/58-0.0722.hdf5\n","\n","Epoch 00059: val_loss did not improve from 0.07223\n","\n","Epoch 00060: val_loss improved from 0.07223 to 0.07130, saving model to ./model/60-0.0713.hdf5\n","\n","Epoch 00061: val_loss improved from 0.07130 to 0.06993, saving model to ./model/61-0.0699.hdf5\n","\n","Epoch 00062: val_loss did not improve from 0.06993\n","\n","Epoch 00063: val_loss did not improve from 0.06993\n","\n","Epoch 00064: val_loss improved from 0.06993 to 0.06819, saving model to ./model/64-0.0682.hdf5\n","\n","Epoch 00065: val_loss improved from 0.06819 to 0.06715, saving model to ./model/65-0.0672.hdf5\n","\n","Epoch 00066: val_loss improved from 0.06715 to 0.06636, saving model to ./model/66-0.0664.hdf5\n","\n","Epoch 00067: val_loss did not improve from 0.06636\n","\n","Epoch 00068: val_loss improved from 0.06636 to 0.06621, saving model to ./model/68-0.0662.hdf5\n","\n","Epoch 00069: val_loss did not improve from 0.06621\n","\n","Epoch 00070: val_loss improved from 0.06621 to 0.06388, saving model to ./model/70-0.0639.hdf5\n","\n","Epoch 00071: val_loss did not improve from 0.06388\n","\n","Epoch 00072: val_loss did not improve from 0.06388\n","\n","Epoch 00073: val_loss improved from 0.06388 to 0.06263, saving model to ./model/73-0.0626.hdf5\n","\n","Epoch 00074: val_loss did not improve from 0.06263\n","\n","Epoch 00075: val_loss did not improve from 0.06263\n","\n","Epoch 00076: val_loss did not improve from 0.06263\n","\n","Epoch 00077: val_loss improved from 0.06263 to 0.06219, saving model to ./model/77-0.0622.hdf5\n","\n","Epoch 00078: val_loss improved from 0.06219 to 0.06176, saving model to ./model/78-0.0618.hdf5\n","\n","Epoch 00079: val_loss improved from 0.06176 to 0.06023, saving model to ./model/79-0.0602.hdf5\n","\n","Epoch 00080: val_loss did not improve from 0.06023\n","\n","Epoch 00081: val_loss did not improve from 0.06023\n","\n","Epoch 00082: val_loss did not improve from 0.06023\n","\n","Epoch 00083: val_loss improved from 0.06023 to 0.05904, saving model to ./model/83-0.0590.hdf5\n","\n","Epoch 00084: val_loss did not improve from 0.05904\n","\n","Epoch 00085: val_loss did not improve from 0.05904\n","\n","Epoch 00086: val_loss improved from 0.05904 to 0.05850, saving model to ./model/86-0.0585.hdf5\n","\n","Epoch 00087: val_loss did not improve from 0.05850\n","\n","Epoch 00088: val_loss improved from 0.05850 to 0.05775, saving model to ./model/88-0.0577.hdf5\n","\n","Epoch 00089: val_loss did not improve from 0.05775\n","\n","Epoch 00090: val_loss did not improve from 0.05775\n","\n","Epoch 00091: val_loss did not improve from 0.05775\n","\n","Epoch 00092: val_loss did not improve from 0.05775\n","\n","Epoch 00093: val_loss did not improve from 0.05775\n","\n","Epoch 00094: val_loss did not improve from 0.05775\n","\n","Epoch 00095: val_loss improved from 0.05775 to 0.05619, saving model to ./model/95-0.0562.hdf5\n","\n","Epoch 00096: val_loss did not improve from 0.05619\n","\n","Epoch 00097: val_loss did not improve from 0.05619\n","\n","Epoch 00098: val_loss did not improve from 0.05619\n","\n","Epoch 00099: val_loss did not improve from 0.05619\n","\n","Epoch 00100: val_loss improved from 0.05619 to 0.05522, saving model to ./model/100-0.0552.hdf5\n","\n","Epoch 00101: val_loss did not improve from 0.05522\n","\n","Epoch 00102: val_loss did not improve from 0.05522\n","\n","Epoch 00103: val_loss improved from 0.05522 to 0.05329, saving model to ./model/103-0.0533.hdf5\n","\n","Epoch 00104: val_loss improved from 0.05329 to 0.05293, saving model to ./model/104-0.0529.hdf5\n","\n","Epoch 00105: val_loss did not improve from 0.05293\n","\n","Epoch 00106: val_loss did not improve from 0.05293\n","\n","Epoch 00107: val_loss improved from 0.05293 to 0.05189, saving model to ./model/107-0.0519.hdf5\n","\n","Epoch 00108: val_loss did not improve from 0.05189\n","\n","Epoch 00109: val_loss did not improve from 0.05189\n","\n","Epoch 00110: val_loss did not improve from 0.05189\n","\n","Epoch 00111: val_loss did not improve from 0.05189\n","\n","Epoch 00112: val_loss did not improve from 0.05189\n","\n","Epoch 00113: val_loss did not improve from 0.05189\n","\n","Epoch 00114: val_loss did not improve from 0.05189\n","\n","Epoch 00115: val_loss did not improve from 0.05189\n","\n","Epoch 00116: val_loss did not improve from 0.05189\n","\n","Epoch 00117: val_loss did not improve from 0.05189\n","\n","Epoch 00118: val_loss improved from 0.05189 to 0.04995, saving model to ./model/118-0.0499.hdf5\n","\n","Epoch 00119: val_loss did not improve from 0.04995\n","\n","Epoch 00120: val_loss improved from 0.04995 to 0.04984, saving model to ./model/120-0.0498.hdf5\n","\n","Epoch 00121: val_loss did not improve from 0.04984\n","\n","Epoch 00122: val_loss did not improve from 0.04984\n","\n","Epoch 00123: val_loss did not improve from 0.04984\n","\n","Epoch 00124: val_loss did not improve from 0.04984\n","\n","Epoch 00125: val_loss improved from 0.04984 to 0.04892, saving model to ./model/125-0.0489.hdf5\n","\n","Epoch 00126: val_loss did not improve from 0.04892\n","\n","Epoch 00127: val_loss did not improve from 0.04892\n","\n","Epoch 00128: val_loss did not improve from 0.04892\n","\n","Epoch 00129: val_loss improved from 0.04892 to 0.04846, saving model to ./model/129-0.0485.hdf5\n","\n","Epoch 00130: val_loss did not improve from 0.04846\n","\n","Epoch 00131: val_loss did not improve from 0.04846\n","\n","Epoch 00132: val_loss did not improve from 0.04846\n","\n","Epoch 00133: val_loss did not improve from 0.04846\n","\n","Epoch 00134: val_loss did not improve from 0.04846\n","\n","Epoch 00135: val_loss did not improve from 0.04846\n","\n","Epoch 00136: val_loss did not improve from 0.04846\n","\n","Epoch 00137: val_loss improved from 0.04846 to 0.04731, saving model to ./model/137-0.0473.hdf5\n","\n","Epoch 00138: val_loss did not improve from 0.04731\n","\n","Epoch 00139: val_loss improved from 0.04731 to 0.04725, saving model to ./model/139-0.0473.hdf5\n","\n","Epoch 00140: val_loss did not improve from 0.04725\n","\n","Epoch 00141: val_loss did not improve from 0.04725\n","\n","Epoch 00142: val_loss did not improve from 0.04725\n","\n","Epoch 00143: val_loss did not improve from 0.04725\n","\n","Epoch 00144: val_loss did not improve from 0.04725\n","\n","Epoch 00145: val_loss did not improve from 0.04725\n","\n","Epoch 00146: val_loss did not improve from 0.04725\n","\n","Epoch 00147: val_loss did not improve from 0.04725\n","\n","Epoch 00148: val_loss did not improve from 0.04725\n","\n","Epoch 00149: val_loss did not improve from 0.04725\n","\n","Epoch 00150: val_loss did not improve from 0.04725\n","\n","Epoch 00151: val_loss did not improve from 0.04725\n","\n","Epoch 00152: val_loss improved from 0.04725 to 0.04554, saving model to ./model/152-0.0455.hdf5\n","\n","Epoch 00153: val_loss did not improve from 0.04554\n","\n","Epoch 00154: val_loss did not improve from 0.04554\n","\n","Epoch 00155: val_loss did not improve from 0.04554\n","\n","Epoch 00156: val_loss improved from 0.04554 to 0.04483, saving model to ./model/156-0.0448.hdf5\n","\n","Epoch 00157: val_loss did not improve from 0.04483\n","\n","Epoch 00158: val_loss did not improve from 0.04483\n","\n","Epoch 00159: val_loss did not improve from 0.04483\n","\n","Epoch 00160: val_loss did not improve from 0.04483\n","\n","Epoch 00161: val_loss did not improve from 0.04483\n","\n","Epoch 00162: val_loss did not improve from 0.04483\n","\n","Epoch 00163: val_loss did not improve from 0.04483\n","\n","Epoch 00164: val_loss did not improve from 0.04483\n","\n","Epoch 00165: val_loss did not improve from 0.04483\n","\n","Epoch 00166: val_loss did not improve from 0.04483\n","\n","Epoch 00167: val_loss did not improve from 0.04483\n","\n","Epoch 00168: val_loss did not improve from 0.04483\n","\n","Epoch 00169: val_loss did not improve from 0.04483\n","\n","Epoch 00170: val_loss did not improve from 0.04483\n","\n","Epoch 00171: val_loss did not improve from 0.04483\n","\n","Epoch 00172: val_loss did not improve from 0.04483\n","\n","Epoch 00173: val_loss did not improve from 0.04483\n","\n","Epoch 00174: val_loss did not improve from 0.04483\n","\n","Epoch 00175: val_loss did not improve from 0.04483\n","\n","Epoch 00176: val_loss did not improve from 0.04483\n","\n","Epoch 00177: val_loss improved from 0.04483 to 0.04405, saving model to ./model/177-0.0441.hdf5\n","\n","Epoch 00178: val_loss did not improve from 0.04405\n","\n","Epoch 00179: val_loss did not improve from 0.04405\n","\n","Epoch 00180: val_loss did not improve from 0.04405\n","\n","Epoch 00181: val_loss did not improve from 0.04405\n","\n","Epoch 00182: val_loss did not improve from 0.04405\n","\n","Epoch 00183: val_loss did not improve from 0.04405\n","\n","Epoch 00184: val_loss did not improve from 0.04405\n","\n","Epoch 00185: val_loss did not improve from 0.04405\n","\n","Epoch 00186: val_loss improved from 0.04405 to 0.04279, saving model to ./model/186-0.0428.hdf5\n","\n","Epoch 00187: val_loss did not improve from 0.04279\n","\n","Epoch 00188: val_loss did not improve from 0.04279\n","\n","Epoch 00189: val_loss did not improve from 0.04279\n","\n","Epoch 00190: val_loss did not improve from 0.04279\n","\n","Epoch 00191: val_loss did not improve from 0.04279\n","\n","Epoch 00192: val_loss did not improve from 0.04279\n","\n","Epoch 00193: val_loss did not improve from 0.04279\n","\n","Epoch 00194: val_loss did not improve from 0.04279\n","\n","Epoch 00195: val_loss did not improve from 0.04279\n","\n","Epoch 00196: val_loss did not improve from 0.04279\n","\n","Epoch 00197: val_loss improved from 0.04279 to 0.04194, saving model to ./model/197-0.0419.hdf5\n","\n","Epoch 00198: val_loss did not improve from 0.04194\n","\n","Epoch 00199: val_loss did not improve from 0.04194\n","\n","Epoch 00200: val_loss did not improve from 0.04194\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fbb7ed14b50>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"aHOE5dawb4xl"},"source":[""],"execution_count":null,"outputs":[]}]}